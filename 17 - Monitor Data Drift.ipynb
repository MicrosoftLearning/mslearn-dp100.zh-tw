{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 監視資料漂移\n",
        "\n",
        "經過一段時間之後，模型在預測的準確性上，可能會因為特性資料趨勢的改變而變差。此現象稱為「資料漂移」**。請務必監視機器學習解決方案來偵測資料漂移，以在必要時重新訓練模型。\n",
        "\n",
        "在此實驗室裡，您將為資料集設定資料漂移監視。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 開始之前\n",
        "\n",
        "除了最新版的 **azureml-sdk** 和 **azureml-widgets** 之外，您還需要 **azureml-datadrift** 套件，才能執行此筆記本的程式碼。請執行下列儲存格，確定是否已安裝。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip show azureml-datadrift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 連線到您的工作區\n",
        "\n",
        "安裝所需的 SDK 套件後，就可以開始連線到工作區。\n",
        "\n",
        "> **注意**：若您尚未使用 Azure 訂閱，建立通過驗證的工作階段，將會提示您按一下連結、輸入驗證碼登入 Azure 來進行驗證。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649368594272
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "# Load the workspace from the saved config file\n",
        "ws = Workspace.from_config()\n",
        "print('Ready to work with', ws.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 建立「基準」**資料集\n",
        "\n",
        "若要監視資料集的資料漂移，必須註冊「基準」**資料集 (通常是用以訓練模型的資料集)，作為未來收集之資料的比較點。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core import Datastore, Dataset\n",
        "from azureml.data.datapath import DataPath\n",
        "\n",
        "# Upload the baseline data\n",
        "default_ds = ws.get_default_datastore()\n",
        "Dataset.File.upload_directory(src_dir='data',\n",
        "                              target=DataPath(default_ds, 'diabetes-data/')\n",
        "                              )\n",
        "\n",
        "# Create and register the baseline dataset\n",
        "print('Registering baseline dataset...')\n",
        "baseline_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-baseline/*.csv'))\n",
        "baseline_data_set = baseline_data_set.register(workspace=ws, \n",
        "                           name='diabetes baseline',\n",
        "                           description='diabetes baseline data',\n",
        "                           tags = {'format':'CSV'},\n",
        "                           create_new_version=True)\n",
        "\n",
        "print('Baseline dataset registered!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 建立「目標」**資料集\n",
        "\n",
        "您可能會在經過一段時間之後，收集到具備基準訓練資料相同特性的新資料。若要比較新資料與基準資料，必須定義目標資料集，在其中加入要用於分析資料漂移的特性，並加入時間戳記欄位，指出新資料當下的時間點。如此一來，您就能測量短期間內的資料漂移。時間戳記可以是資料集本身的欄位，也可以衍生自用以儲存資料的資料夾和檔案名稱樣式。例如，您可能會將新的資料，儲存在以年份命名之資料夾組成的資料夾階層中，資料夾中包含了以月份命名的資料夾，而以月份命名的資料夾，又包含以天命名的資料夾；或者您可能以類似於下列格式，在檔案名稱中加入年、月和日：*data_2020-01-29. csv*。下列程式碼便是採取此方式："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime as dt\n",
        "import pandas as pd\n",
        "\n",
        "print('Generating simulated data...')\n",
        "\n",
        "# Load the smaller of the two data files\n",
        "data = pd.read_csv('data/diabetes2.csv')\n",
        "\n",
        "# We'll generate data for the past 6 weeks\n",
        "weeknos = reversed(range(6))\n",
        "\n",
        "file_paths = []\n",
        "for weekno in weeknos:\n",
        "    \n",
        "    # Get the date X weeks ago\n",
        "    data_date = dt.date.today() - dt.timedelta(weeks=weekno)\n",
        "    \n",
        "    # Modify data to ceate some drift\n",
        "    data['Pregnancies'] = data['Pregnancies'] + 1\n",
        "    data['Age'] = round(data['Age'] * 1.2).astype(int)\n",
        "    data['BMI'] = data['BMI'] * 1.1\n",
        "    \n",
        "    # Save the file with the date encoded in the filename\n",
        "    file_path = 'data/diabetes_{}.csv'.format(data_date.strftime(\"%Y-%m-%d\"))\n",
        "    data.to_csv(file_path)\n",
        "    file_paths.append(file_path)\n",
        "\n",
        "# Upload the files\n",
        "path_on_datastore = 'diabetes-target'\n",
        "default_ds.upload_files(files=file_paths,\n",
        "                       target_path=path_on_datastore,\n",
        "                       overwrite=True,\n",
        "                       show_progress=True)\n",
        "\n",
        "# Use the folder partition format to define a dataset with a 'date' timestamp column\n",
        "partition_format = path_on_datastore + '/diabetes_{date:yyyy-MM-dd}.csv'\n",
        "target_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, path_on_datastore + '/*.csv'),\n",
        "                                                       partition_format=partition_format)\n",
        "\n",
        "# Register the target dataset\n",
        "print('Registering target dataset...')\n",
        "target_data_set = target_data_set.with_timestamp_columns('date').register(workspace=ws,\n",
        "                                                                          name='diabetes target',\n",
        "                                                                          description='diabetes target data',\n",
        "                                                                          tags = {'format':'CSV'},\n",
        "                                                                          create_new_version=True)\n",
        "\n",
        "print('Target dataset registered!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 建立資料漂移監視器\n",
        "\n",
        "您現在已可開始建立糖尿病資料的資料漂移監視器。資料漂移監視器會定期或隨選執行，比較基準資料集與目標資料集，而目標資料集中則會加入一段時間內收集到的新資料。\n",
        "\n",
        "### 建立計算目標\n",
        "\n",
        "若要執行資料漂移監視器，便需要有計算目標。您可以執行下列儲存格來指定計算叢集 (若無此叢集，將會自動建立)。\n",
        "\n",
        "> **重要**：請先將 *your-compute-cluster* 變更成您下列程式碼中的計算叢集名稱，然後再執行！叢集名稱在全域中不可重複，且長度必須介於 2 到 16 個字元。有效字元包括字母、數字及 - 字元。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "cluster_name = \"your-compute-cluster\"\n",
        "\n",
        "try:\n",
        "    # Check for existing compute target\n",
        "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    # If it doesn't already exist, create it\n",
        "    try:\n",
        "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
        "        training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
        "        training_cluster.wait_for_completion(show_output=True)\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **注意**：計算執行個體和叢集採用標準 Azure 虛擬機器映像。此練習建議使用 *Standard_DS11_v2* 映像，以達到成本與效能的最佳平衡。若您訂閱的配額不包含此映像，請選擇備用映像；但請注意，映像愈大，可能產生的費用愈高，然而映像太小，又不足以完成工作。您也可以要求您的 Azure 系統管理員提高您的配額。\n",
        "\n",
        "### 定義資料偏移監視器\n",
        "\n",
        "現在您已準備好使用 **DataDriftDetector**，為您的資料定義資料漂移監視器。您可以指定要監視資料漂移的特性；執行監視流程時所用之計算目標的名稱；比較資料的頻率；觸發警示的資料漂移閾值；以及允許資料收集的等待時間 (小時)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.datadrift import DataDriftDetector\n",
        "\n",
        "# set up feature list\n",
        "features = ['Pregnancies', 'Age', 'BMI']\n",
        "\n",
        "# set up data drift detector\n",
        "monitor = DataDriftDetector.create_from_datasets(ws, 'mslearn-diabates-drift', baseline_data_set, target_data_set,\n",
        "                                                      compute_target=cluster_name, \n",
        "                                                      frequency='Week', \n",
        "                                                      feature_list=features, \n",
        "                                                      drift_threshold=.3, \n",
        "                                                      latency=24)\n",
        "monitor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 回填資料偏移監視器\n",
        "\n",
        "您有一個基準資料集和一個目標資料集，包含了每週的模擬資料收集，共六週。您可以使用此資料集回填監視器，交由其分析原始基準與目標資料之間的資料漂移。\n",
        "\n",
        "> **注意**：因為必須啟動計算目標執行回填分析，所以這可能需要一些時間執行。Widget 不一定會更新其顯示的狀態，因此請按一下連結，觀察 Azure Machine Learning 工作室中的實驗狀態！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.widgets import RunDetails\n",
        "\n",
        "backfill = monitor.backfill(dt.datetime.now() - dt.timedelta(weeks=6), dt.datetime.now())\n",
        "\n",
        "RunDetails(backfill).show()\n",
        "backfill.wait_for_completion()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 分析資料漂移\n",
        "\n",
        "您可以使用下列程式碼，檢查回填執行中所收集之時間點的資料漂移。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "drift_metrics = backfill.get_metrics()\n",
        "for metric in drift_metrics:\n",
        "    print(metric, drift_metrics[metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "您也可以遵循下列步驟，以圖表顯示 [Azure Machine Learning 工作室] (https://ml.azure.com) 中的資料漂移計量：\n",
        "\n",
        "1.在 [資料]**** 頁面上，檢視 [資料集監視器]**** 索引標籤。\n",
        "2.按一下您要檢視的資料漂移監視器。\n",
        "3.選取您要檢視資料漂移計量的日期範圍 (若直條圖未顯示多週的資料，請稍待一分鐘左右的時間，然後按一下 [重新整理]****)。\n",
        "4.檢查頂端 [漂移概觀]**** 區段中的圖表，其中會顯示整體漂移範圍，以及每項特性的漂移比重。\n",
        "5.探索底部 [特性詳細資訊]**** 區段中的圖表，從中了解個別特性的各種漂移計量。\n",
        "\n",
        "> **注意**：若要了解資料漂移計量的詳細資訊，請參閱 Azure Machine Learning 文件中的 ［如何監視資料集］ (https://docs.microsoft.com/zh-tw/azure/machine-learning/how-to-monitor-datasets?tabs=python) (機器翻譯)。\n",
        "\n",
        "## 進一步探索\n",
        "\n",
        "本實驗室的設計在介紹資料漂移監視的概念和原則。若要深入了解如何使用資料集，監視資料漂移的詳細資訊，請參閱 Azure Machine Learning 文件中的 [偵測資料集的資料漂移] (https://docs.microsoft.com/zh-tw/azure/machine-learning/how-to-monitor-datasets?tabs=python) (機器翻譯)。\n",
        "\n",
        "您也可以從已發佈的服務收集資料，作為資料漂移監視的目標資料集。如需詳細資訊，請參閱 [從生產環境中的模型收集資料] (https://docs.microsoft.com/azure/machine-learning/how-to-enable-data-collection) (機器翻譯)。\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
